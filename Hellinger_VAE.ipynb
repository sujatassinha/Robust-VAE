{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers \n",
    "from keras.datasets import mnist \n",
    "from keras import backend as K\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Setup\n",
    "original_dim = 28 * 28\n",
    "intermediate_dim1 = 256\n",
    "intermediate_dim2 = 64\n",
    "latent_dim = 2\n",
    "beta = 0.5           # beta is the weight for penalizer, in paper, it is lambda\n",
    "#tf.random.set_seed(124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(original_dim,))\n",
    "h = layers.Dense(intermediate_dim1, activation='relu')(inputs)\n",
    "# h = layers.Dense(intermediate_dim2, activation='relu')(h)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_sigma = layers.Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample new data points from latent space (re-parameterization trick):\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    # dimension of epsilon: batch_size by 2 in this example\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=1.0)                       \n",
    "    return z_mean + K.exp(0.5*z_log_sigma) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder:\n",
    "encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder:\n",
    "latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(intermediate_dim1, activation='relu')(latent_inputs)\n",
    "x = layers.Dense(intermediate_dim2, activation='relu')(x)\n",
    "outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = keras.Model(latent_inputs, outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model:\n",
    "outputs = decoder(encoder(inputs)[2]) # get latent vector z\n",
    "vae = keras.Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to write Bernoulli distribution for p_theta(x|z)\n",
    "reconstruction_loss = inputs*tf.math.log(1e-10+outputs) + (1-inputs)*tf.math.log(1e-10+1-outputs)\n",
    "reconstruction_loss = tf.reduce_sum(reconstruction_loss, axis=1)\n",
    "\n",
    "# # reconstruction loss: assuming Normal distribution for p_theta(x|z)\n",
    "#mse = -0.5*K.sum(K.square((outputs-mu)/K.exp(logsigma)),axis=1)\n",
    "#sigma_trace = -K.sum(logsigma, axis=1)\n",
    "#log2pi = -0.5*n_dims*np.log(2*np.pi)\n",
    "\n",
    "# A. divergence loss: KL_loss\n",
    "#kl_loss = 0.5*K.sum(K.square(z_mean) + K.exp(z_log_sigma) - 1 - z_log_sigma, axis = -1)\n",
    "# # total loss (combine with first term)\n",
    "#vae_loss = K.mean(-reconstruction_loss + beta*kl_loss)\n",
    "\n",
    "# A_prime another way (Monte Carlo method) to calculate KL divergence (also works)\n",
    "log_diff_p_qphi = 0.5*(-K.square(z) + z_log_sigma + K.square(z-z_mean)/K.exp(z_log_sigma))\n",
    "kl_loss = -K.sum(log_diff_p_qphi, axis = 1)\n",
    "# # total loss (combine with first term)\n",
    "vae_loss = K.mean(-reconstruction_loss + beta*kl_loss)\n",
    "\n",
    "# B divergence loss: HD_loss: doesn't use closed form\n",
    "#log_diff_p_qphi = 0.5*(-K.square(z) + z_log_sigma + K.square(z-z_mean)/K.exp(z_log_sigma))\n",
    "#Aff_loss = K.exp(0.5*K.sum(log_diff_p_qphi, axis = 1))\n",
    "# # B_prime divergence loss: HD loss: closed form\n",
    "#Aff_loss = K.pow(K.prod(K.exp(z_log_sigma), axis = 1), 0.25)/K.sqrt(K.prod((K.exp(z_log_sigma)+1)/2, axis = 1))*K.exp(-0.25*K.sum(K.square(z_mean)/(K.exp(z_log_sigma)+1) ,axis = 1)) \n",
    "#Aff_loss = K.sqrt(K.prod(K.exp(0.5*z_log_sigma)/(K.exp(z_log_sigma)+1), axis = 1))*K.exp(-0.25*K.sum(K.square(z_mean)/(K.exp(z_log_sigma)+1) ,axis = 1)) \n",
    "# # HD total loss (combine with first term)\n",
    "#vae_loss = K.mean(-reconstruction_loss + beta*(1-Aff_loss))\n",
    "\n",
    "# C divergence loss: VNED loss\n",
    "#log_diff_p_qphi = 0.5*(-K.square(z) + z_log_sigma + K.square(z-z_mean)/K.exp(z_log_sigma))\n",
    "#VNED_loss = K.exp(-K.exp(K.sum(log_diff_p_qphi, axis=1))+1) - 1 \n",
    "#VNED_loss = beta*VNED_loss\n",
    "# VNED total loss:\n",
    "#vae_loss = K.mean(-reconstruction_loss + VNED_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "# for the following: len(x_train) = 6000, np.prod(x_train.shape[1:]) = 784\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) \n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i  in range(60000):\n",
    "    #pix =  random.sample(range(784), 78)\n",
    "    # outliers\n",
    "    #z = np.abs(stats.zscore(x_train[i]))\n",
    "    #outliers = np.where(z>2)\n",
    "    # randomly pick outlier location\n",
    "    #random_outlier = random.choice(outliers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#print(z.shape)\n",
    "#print(x_train[1].shape)\n",
    "#x_train[1][155] \n",
    "#print(random.choice(outlier[0]))\n",
    "#print(z[random.choice(outlier[0])])\n",
    "#print(x_train[1][random.choice(outlier[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import random\\nfrom statistics import median\\n# at the locations stored in replace_pixel_location, replace pixel values with outliers (at those locations)\\nfor ar in range(60000):\\n    # list of 78 random number between 0 to 783 without duplicates\\n    replace_pixel_location = random.sample(range(784), 78)\\n    # list of outliers \\n    z = np.abs(stats.zscore(x_train[ar]))\\n    outlier = np.where(z >= 1)\\n    \\n    ##############################################################################################\\n    z_list = []\\n    # list of all the z_values with outlier location\\n    for i in range(len(outlier[0])):\\n        z_list.append(z[outlier[0][i]])\\n    \\n    x_new = x_train[ar][outlier[0][z_list.index(max(z_list))]]\\n    ##############################################################################################\\n    #print(x_new)\\n    # replacing 78 locations with an outlier value\\n    for loc in replace_pixel_location:\\n        # replace pixels with outliers\\n        #x_train[ar][loc] = x_new\\n        #x_train[ar][loc] = x_train[ar][random.choice(outlier[0])]\\n        #x_train[ar][loc] = np.max(x_train[ar]) # to replace x_train[ar][loc] with maximum value\\n        x_train[ar][loc] = random.random() # to replace x_train[ar][loc] with random value between 0 and 1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from statistics import median\n",
    "# at the locations stored in replace_pixel_location, replace pixel values with outliers (at those locations)\n",
    "for ar in range(60000):\n",
    "    # list of 78 random number between 0 to 783 without duplicates\n",
    "    replace_pixel_location = random.sample(range(784), 78)\n",
    "    # list of outliers \n",
    "    z = np.abs(stats.zscore(x_train[ar]))\n",
    "    outlier = np.where(z >= 1)\n",
    "    \n",
    "    ##############################################################################################\n",
    "    z_list = []\n",
    "    # list of all the z_values with outlier location\n",
    "    for i in range(len(outlier[0])):\n",
    "        z_list.append(z[outlier[0][i]])\n",
    "    \n",
    "    x_new = x_train[ar][outlier[0][z_list.index(max(z_list))]]\n",
    "    ##############################################################################################\n",
    "    #print(x_new)\n",
    "    # replacing 78 locations with an outlier value\n",
    "    for loc in replace_pixel_location:\n",
    "        # replace pixels with outliers\n",
    "        #x_train[ar][loc] = x_new\n",
    "        #x_train[ar][loc] = x_train[ar][random.choice(outlier[0])]\n",
    "        #x_train[ar][loc] = np.max(x_train[ar]) # to replace x_train[ar][loc] with maximum value\n",
    "        x_train[ar][loc] = random.random() # to replace x_train[ar][loc] with random value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(my_optimizer):\n",
    "    vae.add_loss(vae_loss)\n",
    "    #my_optimizer = \"Adam\"\n",
    "    vae.compile(optimizer=my_optimizer)  # choices: RMSprop, SGD, Adam\n",
    "    # Another way for optimization (can specify learning rate)\n",
    "    #opt = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "    #vae.compile(optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(f_name):\n",
    "    plt.plot(vae_fit.history['val_loss'])\n",
    "    plt.savefig(f_name, format='png', dpi = 120)\n",
    "    #plt.savefig('Validation Loss Adam MNIST', format='png', dpi = 120)\n",
    "    #plt.savefig('Validation Loss RMSprop MNIST', format='png', dpi = 120)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the VAE is a generative model, we can also use it to generate new digits! \n",
    "# Here we will scan the latent plane, sampling latent points at regular intervals, \n",
    "# and generating the corresponding digit for each of these points. This gives us \n",
    "# a visualization of the latent manifold that \"generates\" the MNIST digits.  \n",
    "\n",
    "def generate_new_img(f_name):\n",
    "    # Display a 2D manifold of the digits\n",
    "    n = 15  # figure with 15x15 digits\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # We will sample n points within [-15, 15] standard deviations\n",
    "    grid_x = np.linspace(-15, 15, n)\n",
    "    grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            # z_sample = np.array([[xi, yi]])\n",
    "            z_sample = np.random.normal(0., 3, size = [1, latent_dim]) # for random order\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure)\n",
    "    plt.savefig(f_name, format='png', dpi = 120)\n",
    "    #plt.savefig('Generating New Images Adam MNIST', format='png', dpi = 120)\n",
    "    #plt.savefig('Generating New Images RMSprop MNIST', format='png', dpi = 120)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to create a mnist picture:\n",
    "def plot_latent_space(decoder, n=15, figsize=15):\n",
    "    # np.random.seed(1)  \n",
    "    # display a n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    scale = 1.0\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            # z_sample = np.array([[xi, yi]])                  # for fixed order\n",
    "            z_sample = np.random.normal(0., 5, size = [1, latent_dim]) # for random order\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.savefig(fig_name, format='png', dpi = 120)\n",
    "    #plt.savefig('Generating New Images (2) Adam MNIST', format='png', dpi = 120)\n",
    "    #plt.savefig('Generating New Images (2) RMSprop MNIST', format='png', dpi = 120)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display how the latent space clusters different digit classes\n",
    "def plot_label_clusters(encoder, data, labels, fname):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(data)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(fname, format='png', dpi = 120)\n",
    "    #plt.savefig('Label Clusters Adam MNIST', format='png', dpi = 120)\n",
    "    #plt.savefig('Label Clusters RMSprop MNIST', format='png', dpi = 120)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "def plot_roc_curve(x_test, tname):\n",
    "    ground_truth_labels = x_test.ravel()\n",
    "    x_score = vae.predict(x_test)\n",
    "    x_score = np.where(x_score < 0.01, 0, x_score)\n",
    "    score_value = x_score.round().ravel()\n",
    "    fpr, tpr, _ = roc_curve(score_value, ground_truth_labels)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(tname)\n",
    "    ax.legend(loc = 'lower right')\n",
    "    plt.savefig(tname, format='png', dpi = 120)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "600/600 [==============================] - 3s 3ms/step - loss: 386.9319 - val_loss: 318.7433\n",
      "Epoch 2/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 325.2461 - val_loss: 308.9153\n",
      "Epoch 3/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 322.0236 - val_loss: 306.0564\n",
      "Epoch 4/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 333.2237 - val_loss: 330.0254\n",
      "Epoch 5/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 327.7843 - val_loss: 322.9591\n",
      "Epoch 6/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 316.7971 - val_loss: 312.6121\n",
      "Epoch 7/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 327.5850 - val_loss: 353.3672\n",
      "Epoch 8/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 324.2445 - val_loss: 342.3284\n",
      "Epoch 9/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 329.6202 - val_loss: 324.4045\n",
      "Epoch 10/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 347.7736 - val_loss: 344.0268\n",
      "Epoch 11/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 347.6014 - val_loss: 358.0166\n",
      "Epoch 12/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 353.7896 - val_loss: 335.9007\n",
      "Epoch 13/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 341.4760 - val_loss: 386.0140\n",
      "Epoch 14/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 351.2516 - val_loss: 317.4984\n",
      "Epoch 15/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 344.8703 - val_loss: 344.2902\n",
      "Epoch 16/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 354.7381 - val_loss: 347.8286\n",
      "Epoch 17/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 355.1892 - val_loss: 347.7005\n",
      "Epoch 18/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 352.7061 - val_loss: 388.2786\n",
      "Epoch 19/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 366.2338 - val_loss: 367.9658\n",
      "Epoch 20/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 355.4976 - val_loss: 347.3562\n",
      "Epoch 21/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 346.7925 - val_loss: 350.2128\n",
      "Epoch 22/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 338.7422 - val_loss: 316.8415\n",
      "Epoch 23/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 325.3859 - val_loss: 343.9315\n",
      "Epoch 24/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 367.6322 - val_loss: 348.5838\n",
      "Epoch 25/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 352.7276 - val_loss: 344.7440\n",
      "Epoch 26/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 353.7605 - val_loss: 358.4857\n",
      "Epoch 27/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 352.7467 - val_loss: 327.2316\n",
      "Epoch 28/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 323.1069 - val_loss: 319.3366\n",
      "Epoch 29/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 346.1638 - val_loss: 370.7019\n",
      "Epoch 30/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 355.5347 - val_loss: 365.4131\n",
      "Epoch 31/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 351.4198 - val_loss: 349.4265\n",
      "Epoch 32/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 348.4958 - val_loss: 344.8261\n",
      "Epoch 33/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 354.9430 - val_loss: 355.7463\n",
      "Epoch 34/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 359.4923 - val_loss: 346.1400\n",
      "Epoch 35/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 354.5518 - val_loss: 342.4025\n",
      "Epoch 36/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 354.2904 - val_loss: 340.5464\n",
      "Epoch 37/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 353.9333 - val_loss: 347.3104\n",
      "Epoch 38/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 357.4114 - val_loss: 348.7016\n",
      "Epoch 39/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 355.1728 - val_loss: 348.3914\n",
      "Epoch 40/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 352.9261 - val_loss: 346.7052\n",
      "Epoch 41/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/200\n",
      "600/600 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 51/200\n",
      "285/600 [=============>................] - ETA: 0s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b7cb8ed629f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m                                                        \u001b[0;31m#20, 100, 200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m vae_fit = vae.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m      6\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#address = '/home/kiara/Desktop/VAE/dr_li_code/Report/VAE/VAE MNIST/lambda=' + str(beta) + '/' + my_optimizer + '/' + l + '/' + number_of_epochs + ' epochs/'                                                                \n",
    "my_optimizer = \"SGD\" #Adam, SGD, RMSprop\n",
    "n_epochs = 200                                                        #20, 100, 200\n",
    "compile_model(my_optimizer)\n",
    "vae_fit = vae.fit(x_train, y_train,\n",
    "                  epochs=n_epochs,\n",
    "                  batch_size=100,\n",
    "                  validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = 'Validation Loss '+ my_optimizer+' MNIST' \n",
    "plot_loss(str1)\n",
    "str2 = 'Generating New Images ' + my_optimizer + ' MNIST' \n",
    "generate_new_img(str2)\n",
    "fig_name = 'Generating New Images (2) ' + my_optimizer + ' MNIST' \n",
    "plot_latent_space(decoder)\n",
    "str3 = 'Label Clusters ' + my_optimizer + ' MNIST'\n",
    "plot_label_clusters(encoder, x_train, y_train, fname = str3)\n",
    "str4 = 'Receiver Operating Characteristic (ROC) Curve ' + my_optimizer + ' MNIST'\n",
    "plot_roc_curve(x_test, str4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
